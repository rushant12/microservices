package com.nedbank.kafka.filemanage.service;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.nedbank.kafka.filemanage.model.*;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.*;

@Service
public class KafkaListenerService {

    private static final Logger logger = LoggerFactory.getLogger(KafkaListenerService.class);

    private final KafkaTemplate<String, String> kafkaTemplate;
    private final BlobStorageService blobStorageService;

    @Value("${kafka.topic.input}")
    private String inputTopic;

    @Value("${kafka.topic.output}")
    private String outputTopic;

    public KafkaListenerService(KafkaTemplate<String, String> kafkaTemplate, BlobStorageService blobStorageService) {
        this.kafkaTemplate = kafkaTemplate;
        this.blobStorageService = blobStorageService;
    }

    @KafkaListener(topics = "${kafka.topic.input}", groupId = "${kafka.consumer.group.id}")
    public void consumeKafkaMessage(ConsumerRecord<String, String> record) {
        String message = record.value();
        logger.info("Received Kafka message: {}", message);

        try {
            String batchId = extractField(message, "batchId");
            String filePath = extractField(message, "filePath");

            logger.info("Parsed batchId: {}, filePath: {}", batchId, filePath);

            String blobUrl = blobStorageService.uploadFile(filePath, batchId);
            logger.info("File uploaded to blob storage at URL: {}", blobUrl);

            Map<String, Object> summaryPayload = buildSummaryPayload(batchId, blobUrl);
            String summaryMessage = new ObjectMapper().writeValueAsString(summaryPayload);

            kafkaTemplate.send(outputTopic, batchId, summaryMessage);
            logger.info("Summary published to Kafka topic: {} with message: {}", outputTopic, summaryMessage);

        } catch (Exception e) {
            logger.error("Error processing Kafka message: {}", e.getMessage(), e);
        }
    }

    private String extractField(String json, String fieldName) {
        ObjectMapper mapper = new ObjectMapper();
        try {
            return mapper.readTree(json).get(fieldName).asText();
        } catch (Exception e) {
            throw new RuntimeException("Failed to extract " + fieldName + " from message: " + json, e);
        }
    }

    private Map<String, Object> buildSummaryPayload(String batchId, String blobUrl) {
        HeaderInfo header = new HeaderInfo();
        MetadataInfo metadata = new MetadataInfo();
        PayloadInfo payload = new PayloadInfo();

        List<ProcessedFileInfo> processedFiles = List.of(
            new ProcessedFileInfo("C001", blobUrl + "/pdfs/C001_" + batchId + ".pdf"),
            new ProcessedFileInfo("C002", blobUrl + "/pdfs/C002_" + batchId + ".pdf")
        );

        SummaryPayload summary = new SummaryPayload();
        summary.setBatchID(batchId);
        summary.setHeader(header);
        summary.setMetadata(metadata);
        summary.setPayload(payload);
        summary.setProcessedFiles(processedFiles);
        summary.setSummaryFileURL(blobUrl + "/summary/" + batchId + "_summary.json");

        ObjectMapper mapper = new ObjectMapper();
        return mapper.convertValue(summary, Map.class);
    }

    public void consumeMessageAndStoreFile(String message) {
        consumeKafkaMessage(new ConsumerRecord<>("manual", 0, 0, null, message));
    }
}
package com.nedbank.kafka.filemanage.config;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;

import java.util.HashMap;
import java.util.Map;

@Configuration
@EnableKafka
public class KafkaConfig {

    @Value("${kafka.bootstrap.servers}")
    private String bootstrapServers;

    @Value("${kafka.consumer.group.id}")
    private String groupId;

    @Value("${kafka.consumer.ssl.keystore.location}")
    private String keystoreLocation;

    @Value("${kafka.consumer.ssl.keystore.password}")
    private String keystorePassword;

    @Value("${kafka.consumer.ssl.key.password}")
    private String keyPassword;

    @Value("${kafka.consumer.ssl.truststore.location}")
    private String truststoreLocation;

    @Value("${kafka.consumer.ssl.truststore.password}")
    private String truststorePassword;

    @Value("${kafka.consumer.ssl.protocol}")
    private String sslProtocol;

    @Bean
    public ConsumerFactory<String, String> consumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put("security.protocol", "SSL");
        props.put("ssl.keystore.location", keystoreLocation);
        props.put("ssl.keystore.password", keystorePassword);
        props.put("ssl.key.password", keyPassword);
        props.put("ssl.truststore.location", truststoreLocation);
        props.put("ssl.truststore.password", truststorePassword);
        props.put("ssl.protocol", sslProtocol);
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        return factory;
    }

    @Bean
    public ProducerFactory<String, String> producerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put("security.protocol", "SSL");
        props.put("ssl.keystore.location", keystoreLocation);
        props.put("ssl.keystore.password", keystorePassword);
        props.put("ssl.key.password", keyPassword);
        props.put("ssl.truststore.location", truststoreLocation);
        props.put("ssl.truststore.password", truststorePassword);
        props.put("ssl.protocol", sslProtocol);
        return new DefaultKafkaProducerFactory<>(props);
    }

    @Bean
    public KafkaTemplate<String, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}

package com.nedbank.kafka.filemanage.model;

import lombok.Data;
import java.util.Date;
import java.util.UUID;

@Data
public class HeaderInfo {
    private String tenantCode = "ZANBL";
    private String channelID = "100";
    private String audienceID = UUID.randomUUID().toString();
    private String timestamp = new Date().toString();
    private String sourceSystem = "CARD";
    private String product = "CASA";
    private String jobName = "SMM815";
}
package com.nedbank.kafka.filemanage.model;

import lombok.Data;

@Data
public class MetadataInfo {
    private int totalFilesProcessed = 2;
    private String processingStatus = "Success";
    private String eventOutcomeCode = "Success";
    private String eventOutcomeDescription = "All customer PDFs processed successfully";
}
package com.nedbank.kafka.filemanage.model;

import lombok.Data;

import java.util.List;
import java.util.UUID;

@Data
public class PayloadInfo {
    private String uniqueConsumerRef = UUID.randomUUID().toString();
    private String uniqueECPBatchRef = UUID.randomUUID().toString();
    private List<String> filenetObjectID = List.of(
        "C044A38E-0000-C21B-B1E2-69FEE895A17B",
        "D8EFC5A4-0000-B22A-B3D5-74FEE895A17B"
    );
    private String repositoryID = "Legacy";
    private String runPriority = "High";
    private String eventID = "E12345";
    private String eventType = "Completion";
    private String restartKey = "Key12345";
}
package com.nedbank.kafka.filemanage.model;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@NoArgsConstructor
@AllArgsConstructor
public class ProcessedFileInfo {
    private String customerID;
    private String pdfFileURL;
}
package com.nedbank.kafka.filemanage.model;

import lombok.Data;

import java.util.Date;
import java.util.List;

@Data
public class SummaryPayload {
    private String batchID;
    private HeaderInfo header;
    private MetadataInfo metadata;
    private PayloadInfo payload;
    private List<ProcessedFileInfo> processedFiles;
    private String summaryFileURL;
    private String timestamp = new Date().toString();
}
